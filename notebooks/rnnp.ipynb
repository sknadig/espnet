{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from glob import glob\n",
    "from espnet.nets.pytorch_backend.e2e_asr import pad_list\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import torch.nn.functional as F\n",
    "from espnet.nets.pytorch_backend.nets_utils import get_subsample\n",
    "\n",
    "import logging\n",
    "import six\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "\n",
    "from espnet.nets.e2e_asr_common import get_vgg2l_odim\n",
    "from espnet.nets.pytorch_backend.nets_utils import make_pad_mask\n",
    "from espnet.nets.pytorch_backend.nets_utils import to_device\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNP(torch.nn.Module):\n",
    "    \"\"\"RNN with projection layer module\n",
    "\n",
    "    :param int idim: dimension of inputs\n",
    "    :param int elayers: number of encoder layers\n",
    "    :param int cdim: number of rnn units (resulted in cdim * 2 if bidirectional)\n",
    "    :param int hdim: number of projection units\n",
    "    :param np.ndarray subsample: list of subsampling numbers\n",
    "    :param float dropout: dropout rate\n",
    "    :param str typ: The RNN type\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, idim, elayers, cdim, hdim, subsample, dropout, typ=\"blstm\"):\n",
    "        super(RNNP, self).__init__()\n",
    "        bidir = typ[0] == \"b\"\n",
    "        for i in six.moves.range(elayers):\n",
    "            if i == 0:\n",
    "                inputdim = idim\n",
    "            else:\n",
    "                inputdim = hdim\n",
    "\n",
    "            RNN = torch.nn.LSTM if \"lstm\" in typ else torch.nn.GRU\n",
    "            rnn = RNN(\n",
    "                inputdim, cdim, num_layers=1, bidirectional=bidir, batch_first=True\n",
    "            )\n",
    "\n",
    "            setattr(self, \"%s%d\" % (\"birnn\" if bidir else \"rnn\", i), rnn)\n",
    "\n",
    "            # bottleneck layer to merge\n",
    "            if bidir:\n",
    "                setattr(self, \"bt%d\" % i, torch.nn.Linear(2 * cdim, hdim))\n",
    "            else:\n",
    "                setattr(self, \"bt%d\" % i, torch.nn.Linear(cdim, hdim))\n",
    "\n",
    "        self.elayers = elayers\n",
    "        self.cdim = cdim\n",
    "        self.subsample = subsample\n",
    "        self.typ = typ\n",
    "        self.bidir = bidir\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, xs_pad, ilens, prev_state=None):\n",
    "        \"\"\"RNNP forward\n",
    "\n",
    "        :param torch.Tensor xs_pad: batch of padded input sequences (B, Tmax, idim)\n",
    "        :param torch.Tensor ilens: batch of lengths of input sequences (B)\n",
    "        :param torch.Tensor prev_state: batch of previous RNN states\n",
    "        :return: batch of hidden state sequences (B, Tmax, hdim)\n",
    "        :rtype: torch.Tensor\n",
    "        \"\"\"\n",
    "        logging.debug(self.__class__.__name__ + \" input lengths: \" + str(ilens))\n",
    "        elayer_states = []\n",
    "        for layer in six.moves.range(self.elayers):\n",
    "            if not isinstance(ilens, torch.Tensor):\n",
    "                ilens = torch.tensor(ilens)\n",
    "            xs_pack = pack_padded_sequence(xs_pad, ilens.cpu(), batch_first=True, enforce_sorted=False)\n",
    "            rnn = getattr(self, (\"birnn\" if self.bidir else \"rnn\") + str(layer))\n",
    "            rnn.flatten_parameters()\n",
    "            if prev_state is not None and rnn.bidirectional:\n",
    "                prev_state = reset_backward_rnn_state(prev_state)\n",
    "            ys, states = rnn(\n",
    "                xs_pack, hx=None if prev_state is None else prev_state[layer]\n",
    "            )\n",
    "            elayer_states.append(states)\n",
    "            # ys: utt list of frame x cdim x 2 (2: means bidirectional)\n",
    "            ys_pad, ilens = pad_packed_sequence(ys, batch_first=True)\n",
    "            sub = self.subsample[layer + 1]\n",
    "            if sub > 1:\n",
    "                ys_pad = ys_pad[:, ::sub]\n",
    "                ilens = torch.tensor([int(i + 1) // sub for i in ilens])\n",
    "            # (sum _utt frame_utt) x dim\n",
    "            projection_layer = getattr(self, \"bt%d\" % layer)\n",
    "            projected = projection_layer(ys_pad.contiguous().view(-1, ys_pad.size(2)))\n",
    "            xs_pad = projected.view(ys_pad.size(0), ys_pad.size(1), -1)\n",
    "            if layer < self.elayers - 1:\n",
    "                xs_pad = torch.tanh(F.dropout(xs_pad, p=self.dropout))\n",
    "\n",
    "        return xs_pad, ilens, elayer_states  # x: utt list of frame x dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASRPerfMonitorDataset(Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.npy_files = glob(f'{data_dir}/*.npy')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.npy_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        contents = np.load(self.npy_files[idx], allow_pickle = True)\n",
    "        x = torch.from_numpy(contents.item().get('logits_history'))\n",
    "        y = torch.tensor(contents.item().get('cer')[0])\n",
    "        sample = {'logits': x, 'cer': y}\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "asr_perf_dataset = ASRPerfMonitorDataset('/home/neo/Desktop/projects/perf_monitor/train/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify_features(batch):\n",
    "    ilens = torch.from_numpy(np.array([np.array(len(x['logits'])) for x in batch]))\n",
    "    \n",
    "    xs_pad = pad_list([x['logits'].float() for x in batch], 0).to(\n",
    "                torch.device('cuda:0'), dtype=torch.float32\n",
    "            )\n",
    "#     xs_packed = pack_padded_sequence(xs_pad, ilens, batch_first=False, enforce_sorted=False)\n",
    "#     ys_pad = np.array([])\n",
    "    ys_pad = torch.stack([torch.tensor(x['cer']) for x in batch]).to('cuda:0')\n",
    "    \n",
    "    return xs_pad, ilens, ys_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(asr_perf_dataset, batch_size=32,\n",
    "                        shuffle=True, num_workers=0, collate_fn=batchify_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNPredictor(torch.nn.Module):\n",
    "    def __init__(self, idim, elayers, cdim, hdim, subsample, dropout, typ=\"blstm\"):\n",
    "        super(RNNPredictor, self).__init__()\n",
    "        self.rnn = RNNP(idim, elayers, cdim, hdim, subsample, dropout, typ=typ)\n",
    "        self.out = torch.nn.Linear(hdim, 1)\n",
    "        \n",
    "    def forward(self, xs_pad, ilens):\n",
    "        hs_pad, hlens, _ = self.rnn(xs_pad, ilens)\n",
    "        means = torch.mean(hs_pad, axis=1)\n",
    "        out = F.relu(self.out(means))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNPredictor(42, 2, 300, 300, [1,1,1], 0.2, 'blstm').to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adadelta(model.parameters(), lr=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-3f981cbbc551>:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ys_pad = torch.stack([torch.tensor(x['cer']) for x in batch]).to('cuda:0')\n",
      "<ipython-input-10-4e9112dd028f>:5: UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(out, sample_batched[2])\n",
      "<ipython-input-10-4e9112dd028f>:5: UserWarning: Using a target size (torch.Size([16])) that is different to the input size (torch.Size([16, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  loss = F.mse_loss(out, sample_batched[2])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1\n",
      "Training epoch 2\n",
      "Training epoch 3\n",
      "Training epoch 4\n",
      "Training epoch 5\n",
      "Training epoch 6\n",
      "Training epoch 7\n",
      "Training epoch 8\n",
      "Training epoch 9\n",
      "Training epoch 10\n",
      "Training epoch 11\n",
      "Training epoch 12\n",
      "Training epoch 13\n",
      "Training epoch 14\n",
      "Training epoch 15\n",
      "Training epoch 16\n",
      "Training epoch 17\n",
      "Training epoch 18\n",
      "Training epoch 19\n",
      "Training epoch 20\n",
      "Training epoch 21\n",
      "Training epoch 22\n",
      "Training epoch 23\n",
      "Training epoch 24\n",
      "Training epoch 25\n",
      "Training epoch 26\n",
      "Training epoch 27\n",
      "Training epoch 28\n",
      "Training epoch 29\n",
      "Training epoch 30\n",
      "Training epoch 31\n",
      "Training epoch 32\n",
      "Training epoch 33\n",
      "Training epoch 34\n",
      "Training epoch 35\n",
      "Training epoch 36\n",
      "Training epoch 37\n",
      "Training epoch 38\n",
      "Training epoch 39\n",
      "Training epoch 40\n",
      "Training epoch 41\n",
      "Training epoch 42\n",
      "Training epoch 43\n",
      "Training epoch 44\n",
      "Training epoch 45\n",
      "Training epoch 46\n",
      "Training epoch 47\n",
      "Training epoch 48\n",
      "Training epoch 49\n",
      "Training epoch 50\n",
      "Training epoch 51\n",
      "Training epoch 52\n",
      "Training epoch 53\n",
      "Training epoch 54\n",
      "Training epoch 55\n",
      "Training epoch 56\n",
      "Training epoch 57\n",
      "Training epoch 58\n",
      "Training epoch 59\n",
      "Training epoch 60\n",
      "Training epoch 61\n",
      "Training epoch 62\n",
      "Training epoch 63\n",
      "Training epoch 64\n",
      "Training epoch 65\n",
      "Training epoch 66\n",
      "Training epoch 67\n",
      "Training epoch 68\n",
      "Training epoch 69\n",
      "Training epoch 70\n",
      "Training epoch 71\n",
      "Training epoch 72\n",
      "Training epoch 73\n",
      "Training epoch 74\n",
      "Training epoch 75\n",
      "Training epoch 76\n",
      "Training epoch 77\n",
      "Training epoch 78\n",
      "Training epoch 79\n",
      "Training epoch 80\n",
      "Training epoch 81\n",
      "Training epoch 82\n",
      "Training epoch 83\n",
      "Training epoch 84\n",
      "Training epoch 85\n",
      "Training epoch 86\n",
      "Training epoch 87\n",
      "Training epoch 88\n",
      "Training epoch 89\n",
      "Training epoch 90\n",
      "Training epoch 91\n",
      "Training epoch 92\n",
      "Training epoch 93\n",
      "Training epoch 94\n",
      "Training epoch 95\n",
      "Training epoch 96\n",
      "Training epoch 97\n",
      "Training epoch 98\n",
      "Training epoch 99\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    print(f'Training epoch {epoch}')\n",
    "    for i_batch, sample_batched in enumerate(dataloader):\n",
    "        out = model(sample_batched[0], sample_batched[1])\n",
    "        loss = F.mse_loss(out, sample_batched[2])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = ASRPerfMonitorDataset('/home/neo/Desktop/projects/perf_monitor/dev//')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-51-79e955f8e5c6>:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  feats = torch.tensor(data['logits']).unsqueeze(axis=0).to('cuda:0')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.028474553 0.38983049988746643\n",
      "0.028055321 0.22807016968727112\n",
      "0.02932418 0.21621622145175934\n",
      "0.028470889 0.05000000074505806\n",
      "0.0286349 0.25\n",
      "0.028206632 0.20512820780277252\n",
      "0.029035745 0.1607142835855484\n",
      "0.028424578 0.11428571492433548\n",
      "0.028007157 0.28947368264198303\n",
      "0.028407833 0.1818181872367859\n",
      "0.028904492 0.23255814611911774\n",
      "0.028520118 0.08695652335882187\n",
      "0.028625581 0.13953489065170288\n",
      "0.028126497 0.16129031777381897\n",
      "0.02831223 0.16129031777381897\n",
      "0.028017245 0.25\n",
      "0.028806228 0.24390244483947754\n",
      "0.028404258 0.3720930218696594\n",
      "0.028293114 0.2195121943950653\n",
      "0.028686304 0.1818181872367859\n",
      "0.027368572 0.30000001192092896\n",
      "0.028169872 0.15000000596046448\n",
      "0.028291922 0.13953489065170288\n",
      "0.028669454 0.25806450843811035\n",
      "0.02879885 0.11538461595773697\n",
      "0.028603954 0.21052631735801697\n",
      "0.027938934 0.3055555522441864\n",
      "0.028607542 0.12121212482452393\n",
      "0.028171428 0.17073170840740204\n",
      "0.028929133 0.20370370149612427\n",
      "0.027619027 0.29411765933036804\n",
      "0.02885395 0.24390244483947754\n",
      "0.028792614 0.1111111119389534\n",
      "0.028299693 0.2083333283662796\n",
      "0.028103413 0.2777777910232544\n",
      "0.028142495 0.19230769574642181\n",
      "0.02799853 0.11666666716337204\n",
      "0.028374104 0.17241379618644714\n",
      "0.028295996 0.3947368562221527\n",
      "0.028442817 0.32758620381355286\n",
      "0.028327612 0.1111111119389534\n",
      "0.028447261 0.09677419066429138\n",
      "0.02871443 0.08510638028383255\n",
      "0.027924882 0.20000000298023224\n",
      "0.028771318 0.09090909361839294\n",
      "0.0277921 0.12903225421905518\n",
      "0.028618313 0.1794871836900711\n",
      "0.02764284 0.1794871836900711\n",
      "0.027793562 0.3333333432674408\n",
      "0.028333543 0.1666666716337204\n",
      "0.029307907 0.10256410390138626\n",
      "0.028011592 0.125\n",
      "0.028366098 0.22448979318141937\n",
      "0.028324608 0.11428571492433548\n",
      "0.028118422 0.3214285671710968\n",
      "0.02779939 0.10000000149011612\n",
      "0.028556265 0.056603774428367615\n",
      "0.028440816 0.23076923191547394\n",
      "0.028705273 0.20512820780277252\n",
      "0.028345084 0.239130437374115\n",
      "0.028025968 0.03030303120613098\n",
      "0.027801963 0.13636364042758942\n",
      "0.028907519 0.4000000059604645\n",
      "0.02807054 0.2916666567325592\n",
      "0.028491622 0.2083333283662796\n",
      "0.028506074 0.3103448152542114\n",
      "0.028169367 0.32258063554763794\n",
      "0.028276706 0.17391304671764374\n",
      "0.02706002 0.20000000298023224\n",
      "0.02831613 0.12121212482452393\n",
      "0.028032161 0.2222222238779068\n",
      "0.02821305 0.18518517911434174\n",
      "0.027873825 0.3235294222831726\n",
      "0.029151265 0.3076923191547394\n",
      "0.02833996 0.0555555559694767\n",
      "0.027569938 0.17241379618644714\n",
      "0.02864934 0.21875\n",
      "0.027995199 0.2432432472705841\n",
      "0.028155234 0.09803921729326248\n",
      "0.028559253 0.09375\n",
      "0.027682526 0.17142857611179352\n",
      "0.0291601 0.17241379618644714\n",
      "0.028564822 0.3529411852359772\n",
      "0.02822767 0.25\n",
      "0.027966518 0.2142857164144516\n",
      "0.028522206 0.02857142873108387\n",
      "0.028164815 0.12820513546466827\n",
      "0.02801322 0.3243243098258972\n",
      "0.02796641 0.29629629850387573\n",
      "0.027825484 0.14705882966518402\n",
      "0.028853366 0.10000000149011612\n",
      "0.02777758 0.21875\n",
      "0.027494473 0.08888889104127884\n",
      "0.027915645 0.10000000149011612\n",
      "0.027646132 0.08695652335882187\n",
      "0.027869524 0.35483869910240173\n",
      "0.028315095 0.12121212482452393\n",
      "0.027621867 0.10000000149011612\n",
      "0.028526286 0.17499999701976776\n",
      "0.028627388 0.1875\n",
      "0.028190099 0.08695652335882187\n",
      "0.028288567 0.1818181872367859\n",
      "0.027932448 0.17241379618644714\n",
      "0.028392352 0.4285714328289032\n",
      "0.028302275 0.27586206793785095\n",
      "0.027839111 0.2978723347187042\n",
      "0.028389534 0.10526315867900848\n",
      "0.028515315 0.1090909093618393\n",
      "0.028280118 0.1538461595773697\n",
      "0.027950658 0.07999999821186066\n",
      "0.028701866 0.02380952425301075\n",
      "0.028051693 0.10000000149011612\n",
      "0.027667562 0.1538461595773697\n",
      "0.028200174 0.3513513505458832\n",
      "0.02826725 0.14516128599643707\n",
      "0.028106444 0.24137930572032928\n",
      "0.028697664 0.03333333507180214\n",
      "0.028224135 0.1463414579629898\n",
      "0.02841428 0.02380952425301075\n",
      "0.02842113 0.05263157933950424\n",
      "0.027855994 0.21052631735801697\n",
      "0.028078439 0.3499999940395355\n",
      "0.027655335 0.1666666716337204\n",
      "0.028596291 0.36666667461395264\n",
      "0.028166771 0.30434781312942505\n",
      "0.028602052 0.12121212482452393\n",
      "0.0287716 0.1666666716337204\n",
      "0.028389357 0.23076923191547394\n",
      "0.028121771 0.1621621549129486\n",
      "0.028070457 0.08695652335882187\n",
      "0.028023686 0.2916666567325592\n",
      "0.02879836 0.07692307978868484\n",
      "0.028923973 0.22499999403953552\n",
      "0.028824491 0.25\n",
      "0.027786577 0.27272728085517883\n",
      "0.028499585 0.3333333432674408\n",
      "0.02791622 0.0476190485060215\n",
      "0.027902575 0.125\n",
      "0.02837368 0.25581395626068115\n",
      "0.02826895 0.21739129722118378\n",
      "0.028374989 0.0810810774564743\n",
      "0.027858302 0.2380952388048172\n",
      "0.027771968 0.3125\n",
      "0.028442897 0.0882352963089943\n",
      "0.028764548 0.27906978130340576\n",
      "0.02817684 0.043478261679410934\n",
      "0.02788232 0.21212121844291687\n",
      "0.028622618 0.1818181872367859\n",
      "0.028170401 0.1764705926179886\n",
      "0.029415762 0.20512820780277252\n",
      "0.02849089 0.40909090638160706\n",
      "0.028304916 0.19230769574642181\n",
      "0.028772553 0.17073170840740204\n",
      "0.028437374 0.15217390656471252\n",
      "0.02830967 0.2711864411830902\n",
      "0.028451882 0.25\n",
      "0.029156914 0.16129031777381897\n",
      "0.028649239 0.25\n",
      "0.029077847 0.15909090638160706\n",
      "0.028597455 0.11666666716337204\n",
      "0.028306289 0.05263157933950424\n",
      "0.028855208 0.28205129504203796\n",
      "0.029019726 0.125\n",
      "0.028744927 0.12727272510528564\n",
      "0.028503567 0.1621621549129486\n",
      "0.02844338 0.06896551698446274\n",
      "0.028403528 0.1621621549129486\n",
      "0.02780983 0.2857142984867096\n",
      "0.029128153 0.1627907007932663\n",
      "0.028790006 0.1746031790971756\n",
      "0.028666792 0.22727273404598236\n",
      "0.02821374 0.1489361673593521\n",
      "0.028558565 0.2666666805744171\n",
      "0.02803582 0.15094339847564697\n",
      "0.027644796 0.3333333432674408\n",
      "0.028191447 0.2777777910232544\n",
      "0.028488876 0.06818182021379471\n",
      "0.02785983 0.25\n",
      "0.028271351 0.2777777910232544\n",
      "0.028181061 0.2153846174478531\n",
      "0.027682947 0.25\n",
      "0.028345961 0.09090909361839294\n",
      "0.027879884 0.13333334028720856\n",
      "0.028075667 0.19565217196941376\n",
      "0.028825592 0.23529411852359772\n",
      "0.028026072 0.3529411852359772\n",
      "0.02835994 0.125\n",
      "0.028564462 0.24137930572032928\n",
      "0.028429585 0.095238097012043\n",
      "0.027958605 0.1388888955116272\n",
      "0.027977781 0.15000000596046448\n",
      "0.028716605 0.2537313401699066\n",
      "0.027610384 0.16129031777381897\n",
      "0.028455421 0.1621621549129486\n",
      "0.028449452 0.1111111119389534\n",
      "0.028293062 0.03030303120613098\n",
      "0.02791685 0.25\n",
      "0.027773554 0.35555556416511536\n",
      "0.028133692 0.27272728085517883\n",
      "0.027604168 0.2631579041481018\n",
      "0.028428819 0.31578946113586426\n",
      "0.028003301 0.11999999731779099\n",
      "0.027894381 0.18918919563293457\n",
      "0.028450053 0.2800000011920929\n",
      "0.028986057 0.17073170840740204\n",
      "0.028284289 0.17777778208255768\n",
      "0.028263764 0.10000000149011612\n",
      "0.027761277 0.1818181872367859\n",
      "0.028528292 0.22033898532390594\n",
      "0.028126677 0.21052631735801697\n",
      "0.02775568 0.1304347813129425\n",
      "0.027989393 0.1568627506494522\n",
      "0.0273716 0.09615384787321091\n",
      "0.027961899 0.22727273404598236\n",
      "0.028485086 0.28947368264198303\n",
      "0.027789654 0.2291666716337204\n",
      "0.028762808 0.09677419066429138\n",
      "0.028229319 0.27272728085517883\n",
      "0.028209344 0.380952388048172\n",
      "0.028150365 0.2702702581882477\n",
      "0.02826647 0.1034482792019844\n",
      "0.02832354 0.12121212482452393\n",
      "0.028299913 0.13333334028720856\n",
      "0.028653657 0.1818181872367859\n",
      "0.028250579 0.032258063554763794\n",
      "0.02823653 0.1269841343164444\n",
      "0.027675586 0.3913043439388275\n",
      "0.028315455 0.3125\n",
      "0.028168527 0.3448275923728943\n",
      "0.028631594 0.1794871836900711\n",
      "0.028541753 0.1666666716337204\n",
      "0.028114913 0.16326530277729034\n",
      "0.0280294 0.05714285746216774\n",
      "0.027974524 0.1388888955116272\n",
      "0.028306076 0.1666666716337204\n",
      "0.028609918 0.08571428805589676\n",
      "0.027983647 0.24590164422988892\n",
      "0.029252421 0.1428571492433548\n",
      "0.027597459 0.2222222238779068\n",
      "0.02896617 0.15789473056793213\n",
      "0.028260374 0.22499999403953552\n",
      "0.028804842 0.19565217196941376\n",
      "0.02802529 0.2666666805744171\n",
      "0.028211841 0.36000001430511475\n",
      "0.027771141 0.0833333358168602\n",
      "0.028146071 0.20000000298023224\n",
      "0.02768692 0.06896551698446274\n",
      "0.028095607 0.22807016968727112\n",
      "0.02828186 0.1818181872367859\n",
      "0.028895542 0.18918919563293457\n",
      "0.028058441 0.30000001192092896\n",
      "0.028461777 0.1818181872367859\n",
      "0.028008467 0.23529411852359772\n",
      "0.028130397 0.2857142984867096\n",
      "0.028634436 0.2291666716337204\n",
      "0.027688213 0.16949152946472168\n",
      "0.028427426 0.34285715222358704\n",
      "0.02839131 0.1515151560306549\n",
      "0.028508103 0.18333333730697632\n",
      "0.02816125 0.25\n",
      "0.028828643 0.1034482792019844\n",
      "0.028763961 0.31578946113586426\n",
      "0.028173326 0.15625\n",
      "0.027616404 0.18000000715255737\n",
      "0.027762434 0.29411765933036804\n",
      "0.028324889 0.1538461595773697\n",
      "0.028859237 0.1428571492433548\n",
      "0.02874157 0.1785714328289032\n",
      "0.028141726 0.125\n",
      "0.028442215 0.0625\n",
      "0.028375853 0.07894736528396606\n",
      "0.0290506 0.17142857611179352\n",
      "0.02876385 0.20000000298023224\n",
      "0.027755123 0.1551724076271057\n",
      "0.02835101 0.19230769574642181\n",
      "0.028793847 0.3055555522441864\n",
      "0.028733687 0.1489361673593521\n",
      "0.02816475 0.15000000596046448\n",
      "0.02849539 0.2666666805744171\n",
      "0.028667903 0.20000000298023224\n",
      "0.028357826 0.3617021143436432\n",
      "0.028481677 0.1860465109348297\n",
      "0.028222144 0.2380952388048172\n",
      "0.028231548 0.26923078298568726\n",
      "0.0280725 0.08510638028383255\n",
      "0.02780904 0.1627907007932663\n",
      "0.028359504 0.0555555559694767\n",
      "0.027717572 0.05714285746216774\n",
      "0.028688768 0.16129031777381897\n",
      "0.028312681 0.2380952388048172\n",
      "0.027214522 0.3265306055545807\n",
      "0.028480759 0.3199999928474426\n",
      "0.028097648 0.2222222238779068\n",
      "0.028413842 0.18918919563293457\n",
      "0.028643489 0.20000000298023224\n",
      "0.028328426 0.07692307978868484\n",
      "0.028729532 0.2083333283662796\n",
      "0.028177101 0.11428571492433548\n",
      "0.028443757 0.28947368264198303\n",
      "0.028526954 0.29729729890823364\n",
      "0.028133553 0.20588235557079315\n",
      "0.028791677 0.1666666716337204\n",
      "0.02938669 0.08571428805589676\n",
      "0.027978972 0.095238097012043\n",
      "0.02854597 0.14035087823867798\n",
      "0.028698884 0.07407407462596893\n",
      "0.0282752 0.1818181872367859\n",
      "0.027885007 0.15625\n",
      "0.028366039 0.25\n",
      "0.028103976 0.30000001192092896\n",
      "0.02836791 0.22857142984867096\n",
      "0.028638035 0.2222222238779068\n",
      "0.028946266 0.1818181872367859\n",
      "0.028670607 0.24444444477558136\n",
      "0.027938616 0.27272728085517883\n",
      "0.0284576 0.20000000298023224\n",
      "0.028171342 0.06818182021379471\n",
      "0.028232256 0.22580644488334656\n",
      "0.027968597 0.24242424964904785\n",
      "0.028815877 0.1785714328289032\n",
      "0.028048899 0.10000000149011612\n",
      "0.027944 0.23076923191547394\n",
      "0.028274382 0.15000000596046448\n",
      "0.028151123 0.03125\n",
      "0.02866504 0.20588235557079315\n",
      "0.028059963 0.21739129722118378\n",
      "0.028298521 0.21621622145175934\n",
      "0.028631883 0.18518517911434174\n",
      "0.028582232 0.10204081982374191\n",
      "0.028377727 0.1388888955116272\n",
      "0.02857245 0.10638298094272614\n",
      "0.028732939 0.21052631735801697\n",
      "0.028177831 0.17499999701976776\n",
      "0.027503198 0.2666666805744171\n",
      "0.027882868 0.2800000011920929\n",
      "0.028526653 0.25806450843811035\n",
      "0.027247889 0.17777778208255768\n",
      "0.027740171 0.14516128599643707\n",
      "0.028428609 0.21739129722118378\n",
      "0.027687024 0.23529411852359772\n",
      "0.028447738 0.25\n",
      "0.028050717 0.11764705926179886\n",
      "0.028104708 0.190476194024086\n",
      "0.028397111 0.1538461595773697\n",
      "0.027717724 0.14035087823867798\n",
      "0.027578995 0.1818181872367859\n",
      "0.028310262 0.3636363744735718\n",
      "0.028326552 0.13636364042758942\n",
      "0.027835796 0.1607142835855484\n",
      "0.02874162 0.1304347813129425\n",
      "0.028920196 0.18421052396297455\n",
      "0.027953453 0.13461539149284363\n",
      "0.028020097 0.12121212482452393\n",
      "0.027363589 0.18000000715255737\n",
      "0.027810989 0.3030303120613098\n",
      "0.028630465 0.23529411852359772\n",
      "0.029134143 0.1875\n",
      "0.027994273 0.11999999731779099\n",
      "0.02789018 0.05000000074505806\n",
      "0.028041309 0.0555555559694767\n",
      "0.028309364 0.1538461595773697\n",
      "0.027160423 0.190476194024086\n",
      "0.0286306 0.0\n",
      "0.028415617 0.29411765933036804\n",
      "0.028479258 0.1538461595773697\n",
      "0.02816535 0.14754098653793335\n",
      "0.029100452 0.23404255509376526\n",
      "0.028521378 0.1489361673593521\n",
      "0.027896805 0.20588235557079315\n",
      "0.028558927 0.19696970283985138\n",
      "0.027824774 0.19354838132858276\n",
      "0.028484931 0.2222222238779068\n",
      "0.027563555 0.19230769574642181\n",
      "0.02814077 0.095238097012043\n",
      "0.027966924 0.3125\n",
      "0.028159998 0.19354838132858276\n",
      "0.027860787 0.4333333373069763\n",
      "0.027750298 0.2702702581882477\n",
      "0.028153757 0.1666666716337204\n",
      "0.02850367 0.10810811072587967\n",
      "0.028297529 0.125\n",
      "0.028426064 0.28125\n",
      "0.028461328 0.06896551698446274\n",
      "0.029305976 0.375\n",
      "0.028103765 0.2777777910232544\n",
      "0.028036315 0.1621621549129486\n",
      "0.028620422 0.190476194024086\n",
      "0.027858952 0.13333334028720856\n",
      "0.028707385 0.15094339847564697\n",
      "0.028777014 0.22448979318141937\n",
      "0.028660836 0.19354838132858276\n",
      "0.028557338 0.2068965584039688\n",
      "0.028470632 0.1463414579629898\n",
      "0.027584998 0.17142857611179352\n",
      "0.028037589 0.25806450843811035\n",
      "0.028275255 0.1230769231915474\n",
      "0.028314231 0.2666666805744171\n",
      "0.027894203 0.1666666716337204\n",
      "0.028226173 0.12121212482452393\n",
      "0.028807588 0.23076923191547394\n"
     ]
    }
   ],
   "source": [
    "for idx in range(len(test_dataset.npy_files)):\n",
    "    with torch.no_grad():\n",
    "        data = test_dataset[idx]\n",
    "        feats = torch.tensor(data['logits']).unsqueeze(axis=0).to('cuda:0')\n",
    "        labels = data['cer']\n",
    "        ilens = len(feats)\n",
    "        out = model(feats, [ilens])\n",
    "        print(out.detach().cpu().numpy()[0][0], labels.numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(p, dim = -1, keepdim = None):\n",
    "    return -torch.where(p > 0, p * p.log(), p.new([0.0])).sum(dim = dim, keepdim = keepdim) # can be a scalar, when PyTorch.supports it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = test_dataset[10]['logits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-73-944212d9c055>:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  p = F.softmax(logits[0])\n"
     ]
    }
   ],
   "source": [
    "p = F.softmax(logits[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = p.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "logp = np.log2(p)\n",
    "entropy1 = np.sum(-p*logp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0033528002"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-97-349dd03252d5>:8: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  p = F.softmax(row)\n"
     ]
    }
   ],
   "source": [
    "x = []\n",
    "y = []\n",
    "for idx in range(len(asr_perf_dataset.npy_files)):\n",
    "    data = asr_perf_dataset[idx]\n",
    "    if data['cer'] != 0:\n",
    "        entropy = 0\n",
    "        for row in data['logits']:\n",
    "            p = F.softmax(row)\n",
    "            p = p.numpy()\n",
    "            logp = np.log2(p)\n",
    "            entropy1 = np.sum(-p*logp)\n",
    "            entropy += entropy1\n",
    "        result = entropy / len(data['logits'])\n",
    "        x.append(result)\n",
    "        y.append(data['cer'].numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fa223e6daf0>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x,y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
